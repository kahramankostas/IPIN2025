{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe0c63e",
   "metadata": {},
   "source": [
    "## 2.2  Setting Random Seeds\n",
    "In this step, we fixed the related random seed to be used in order to obtain repeatable results. In this way, we have provided a deterministic path where we get the same result in every run. However, according to our observations, the results obtained on different computers may differ slightly (±1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a03f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2  Setting Random Seeds\n",
    "seed_value=0\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68637ef",
   "metadata": {},
   "source": [
    "## 2.3 Loading the data  \n",
    "In this section, the files given for the train data are loaded.\n",
    "- `wifi`  variable takes IDs and weights from the  `task2_train_estimated_wifi_distances.csv`.\n",
    "- `steps`  variable takes IDs and weights from file `task2_train_steps.csv`.\n",
    "- `elevs`  variable takes IDs from file  `task2_train_elevations.csv`.\n",
    "- `fp_lookup`  variable gets  IDs and trajectories from the  `task2_train_lookup.json` file.\n",
    "\n",
    "\n",
    "We did not prefer the method of recalculating the estimated distances given in wifi with the model we obtained in Task1, because the results we obtained from this process did not make a significant difference. That's why we didn't use the `task2_train_fingerprints.json` file in our ultimate work.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Please make sure there are `task2_train_estimated_wifi_distances.csv`, `task2_train_steps.csv`, `task2_train_elevations.csv`, `task2_train_lookup.json`  files in the `task2_for_participants/train` folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d110c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip required files\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"task2_for_participants.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c08e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944671it [00:04, 456463.28it/s]\n",
      "2224it [00:00, 286701.87it/s]\n",
      "98610it [00:00, 595575.40it/s]\n"
     ]
    }
   ],
   "source": [
    "## 2.3 Loading the data  \n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_to_data = \"task2_for_participants/train\"\n",
    "\n",
    "with open(os.path.join(path_to_data,\"task2_train_estimated_wifi_distances.csv\")) as f:\n",
    "    wifi = []\n",
    "    reader = csv.DictReader(f)\n",
    "    for line in tqdm(reader):\n",
    "        wifi.append([line['id1'],line['id2'],float(line['estimated_distance'])])\n",
    "        \n",
    "with open(os.path.join(path_to_data,\"task2_train_elevations.csv\")) as f:\n",
    "    elevs = []\n",
    "    reader = csv.DictReader(f)\n",
    "    for line in tqdm(reader):\n",
    "        elevs.append([line['id1'],line['id2']])        \n",
    "\n",
    "with open(os.path.join(path_to_data,\"task2_train_steps.csv\")) as f:\n",
    "    steps = []\n",
    "    reader = csv.DictReader(f)\n",
    "    for line in tqdm(reader):\n",
    "        steps.append([line['id1'],line['id2'],float(line['displacement'])]) \n",
    "        \n",
    "fp_lookup_path = os.path.join(path_to_data,\"task2_train_lookup.json\")\n",
    "\n",
    "with open(fp_lookup_path) as f:\n",
    "    fp_lookup = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f304d",
   "metadata": {},
   "source": [
    "## 2.3 Generating the Trajectory  graph. \n",
    "We take the average distance as the weight when creating the Trajectory graph. We used the example given for Task 2 (`Task2-IPS-Challenge-2021.ipynb`) for this process. We saved the resulting graph (`B`) as an adjacency list in the directory (as `my.adjlist`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012ba811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1944671/1944671 [00:02<00:00, 762649.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2224/2224 [00:00<00:00, 1239618.88it/s]\n"
     ]
    }
   ],
   "source": [
    "## 2.3 Generating the Trajectory  graph. \n",
    "B = nx.Graph()\n",
    "\n",
    "# Get all the trajectory ids from the lookup\n",
    "valid_nodes = set(fp_lookup.values())\n",
    "\n",
    "for node in valid_nodes:\n",
    "    B.add_node(node)\n",
    "\n",
    "# Either add an edge or append the distance to the edge data\n",
    "for id1,id2,dist in tqdm(wifi):\n",
    "    if not B.has_edge(fp_lookup[str(id1)], fp_lookup[str(id2)]):\n",
    "        \n",
    "        B.add_edge(fp_lookup[str(id1)], \n",
    "                   fp_lookup[str(id2)], \n",
    "                   ty = \"w\", weight=[dist])\n",
    "    else:\n",
    "        B[fp_lookup[str(id1)]][fp_lookup[str(id2)]]['weight'].append(dist)\n",
    "        \n",
    "# Compute the mean edge weight\n",
    "for edge in B.edges(data=True):\n",
    "    B[edge[0]][edge[1]]['weight'] = sum(B[edge[0]][edge[1]]['weight'])/len(B[edge[0]][edge[1]]['weight'])\n",
    "        \n",
    "# If you have made a wifi connection between trajectories with an elev, delete the edge\n",
    "for id1,id2 in tqdm(elevs):\n",
    "    if B.has_edge(fp_lookup[str(id1)], fp_lookup[str(id2)]):\n",
    "        B.remove_edge(fp_lookup[str(id1)], \n",
    "                      fp_lookup[str(id2)])\n",
    "\n",
    "nx.write_adjlist(B, \"my.adjlist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf45af",
   "metadata": {},
   "source": [
    "## 2.4 Converting nodes to vectors\n",
    "Before giving the adjacency list as input to the machine learning algorithms, we convert the nodes to vector. In our work we used Node2vec as a graph embedding algorithm methodology proposed by Grover&Leskovec in 2016 \\[1\\]. Node2vec is a semi-supervised algorithm for feature learning of the nodes in the network. Node2vec is created based on the skip-gram technique, which is an NLP approach motivated on Distributional Structure concept.According to idea, if different words used in similar contexts they probably have a similar meaning and there is an obvious relationship between them. Skip-gram technique uses center word(input) to predict neighbours(output) while computing the probabilities of surroundings based on a given window size (contiguous sequence of items before of after the center), in other words N-grams. Unlike the NLP approach, Node2vec system is not fed with words which have a linear structure, but nodes and edges, which have a distributed graphical structure. This multidimensional structure makes embeddings complex and computationally expensive, but Node2vec uses negative sampling with optimizing stochastic gradient descent (SGD) to deal with it. In addition to this, The random walk approach is used to detect sample neighboring nodes of the source node in a nonlinear structure. \n",
    "\n",
    "In our study, we first performed the vector representation of node relationships in low-dimensional space by modeling with node2vec from given distance of two nodes (weights). Then we used the output of Node2vec (graph embeddings), which has vectors of nodes, to feed traditional K-means clustering algorithm.\n",
    "\n",
    "The parameters we use in Nod2vec can be listed as follows:\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "| ------ | ------ |\n",
    "|dimensions|32|\n",
    "|walk_length|15|\n",
    "|num_walks|100|\n",
    "|workers|1|\n",
    "|seed|0|\n",
    "|window|10|\n",
    "|min_count|1|\n",
    "|batch_words|4|\n",
    "\n",
    "The Node2vec model takes the adjacency list as input and outputs a 32-size vector. In this part, the node.py file is created and run in the `Jupyter Notebook`. There are two reasons why it is preferable to run externally rather than in a Jupyter notebook cell.\n",
    "- It allows fixing the random seed of the node2vec module. Thus, it ensures repeatability by providing the same result in different runs.\n",
    "- `Node2vec` is a very computationally expensive method, RAM overflow error is quite possible if run inside Jupyter notebook. Creating and running `Node2vec` model outside avoids this error.\n",
    "\n",
    "The cell below creates the file named node.py. This file creates the node2vec model. This model takes the the adjacency list (`my.adjlist`) as input and creates a 32-dimensional vector file as output (`vectors.emb`).\n",
    "\n",
    "**Important! The code below should be run in Linux distributions (Tested in Google colab and Ubuntu).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c7dca21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'PYTHONHASHSEED' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# 2.4 Converting nodes to vectors\n",
    "# A folder named tmp is created. This folder is essential for the node2vec model to use less RAM.\n",
    "try:\n",
    "    if not os.path.exists(\"tmp\"):\n",
    "        os.makedirs(\"tmp\")\n",
    "except OSError:\n",
    "    print (\"The folder could not be created!\\n Please manually create the \\\"tmp\\\" folder in the directory\")\n",
    "\n",
    "\n",
    "node=\"\"\"\n",
    "\n",
    "# importing related modules\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "\n",
    "#importing  adjacency list file as B\n",
    "B = nx.read_adjlist(\"my.adjlist\")\n",
    "\n",
    "seed_value=0\n",
    "\n",
    "\n",
    "# Specifying the input and hyperparameters of the node2vec model\n",
    "node2vec = Node2Vec(B, dimensions=32, walk_length=15, num_walks=100, workers=1,seed=seed_value,temp_folder = './tmp')  \n",
    "\n",
    "#Assigning/specifying random seeds\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "\n",
    "# creation of the model\n",
    "\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4,seed=seed_value)   \n",
    "\n",
    "\n",
    "# saving the output vector\n",
    "\n",
    "model.wv.save_word2vec_format(\"vectors.emb\")\n",
    "\n",
    "# save the model\n",
    "model.save(\"vectorMODEL\")\n",
    "\n",
    "\"\"\"\n",
    "f = open(\"node.py\", \"w\")\n",
    "f.write(node)\n",
    "f.close()\n",
    "\n",
    "\n",
    "!PYTHONHASHSEED=0 python3 node.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a955910-d510-428c-ace9-edf4b779a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python node.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e76a0",
   "metadata": {},
   "source": [
    "## 2.4 Reshaping data\n",
    "After our vector file is created, we read this file (`vectors.emb`). This file consists of 33 columns. The first column is the node number (IDs), and remain are vector values. By sorting the entire file by the first column, we return the nodes to their original order. Then we delete this ID column, which we will no longer use. So, we give the final shape of our data. Our data is ready to be used in machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d55e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of vector file:  (8371, 33)\n",
      "[[ 4.8020000e+03  5.5105895e-01  1.8139237e-01 ...  2.0771873e-01\n",
      "  -3.6370390e-01 -1.5758507e-01]\n",
      " [ 3.2080000e+03  6.6994180e-01  1.8356870e-02 ...  1.6239063e-01\n",
      "  -3.3889225e-01 -3.2257378e-01]\n",
      " [ 3.7450000e+03  8.5945580e-01  1.7559928e-01 ...  3.2490490e-01\n",
      "  -3.5132822e-01 -2.4129300e-01]\n",
      " ...\n",
      " [ 1.3850000e+03 -8.3527340e-03 -2.9195592e-02 ... -2.3357347e-02\n",
      "   2.8256886e-03 -2.4400268e-02]\n",
      " [ 2.7150000e+03 -2.9293872e-02 -2.2110324e-02 ... -2.3724474e-03\n",
      "   1.4017977e-02 -1.6061176e-02]\n",
      " [ 6.5080000e+03 -2.1313433e-02 -2.2092693e-02 ...  1.2072727e-02\n",
      "   1.7553598e-02  1.3892725e-04]]\n"
     ]
    }
   ],
   "source": [
    "# 2.4 Reshaping data\n",
    "vec = np.loadtxt(\"vectors.emb\", skiprows=1)\n",
    "print(\"shape of vector file: \",vec.shape)\n",
    "print(vec)\n",
    "vec=vec[vec[:,0].argsort()]; \n",
    "vec=vec[0:vec.shape[0],1:vec.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f22b10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(n_clusters=17, random_state=10) 2.140610456466675\n"
     ]
    }
   ],
   "source": [
    "# 2.5  Best result\n",
    "from sklearn import cluster\n",
    "import time\n",
    "\n",
    "ML_results = []\n",
    "k_clusters = 17\n",
    "algorithms = {}\n",
    "algorithms['KMeans'] =cluster.KMeans(n_clusters=k_clusters,random_state=10)\n",
    "second=time.time()\n",
    "for model in algorithms.values():\n",
    "    model.fit(vec)\n",
    "    ML_results=list(model.labels_)\n",
    "    print(model,time.time()-second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc6d0c",
   "metadata": {},
   "source": [
    "## 2.6 RESULTS \n",
    "The output of the machine learning algorithm determines which cluster the fingerprints belong to. But what is required of us is to cluster the trajectories. Therefore, these fingerprints are converted to their trajectory counterparts using the `fp_lookup` variable. This output is processed into the `Node2Vec.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86b40502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node2Vec.csv file is ready!\n"
     ]
    }
   ],
   "source": [
    "## 2.6 RESULTS \n",
    "result={}\n",
    "for ii,i in enumerate(set(fp_lookup.values())):\n",
    "    result[i]=ML_results[ii]\n",
    "        \n",
    "    \n",
    "ters={}\n",
    "for i in result:\n",
    "    if result[i] not in ters:\n",
    "        ters[result[i]]=[]\n",
    "        ters[result[i]].append(i)\n",
    "    else:\n",
    "        ters[result[i]].append(i)\n",
    "        \n",
    "        \n",
    "        \n",
    "final_results=[]\n",
    "for i in ters:\n",
    "    final_results.append(ters[i])\n",
    "    \n",
    "    \n",
    "name=\"Node2Vec.csv\"    \n",
    "with open(name, \"w\", newline='') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerows(final_results)\n",
    "print(name, \"file is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434ec38",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba6492-2985-4e3c-9b04-f13215dae6e0",
   "metadata": {},
   "source": [
    "## Runs the specified community detection algorithm and returns the results\n",
    "- Fast Greedy\n",
    "- Infomap\n",
    "- Label Prop.\n",
    "- Leiden\n",
    "- Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e27fd5a-742d-4595-8ac8-baa551c8e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import leidenalg\n",
    "import igraph as ig\n",
    "import time\n",
    "import concurrent.futures\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def run_algorithm(G_ig, algorithm_name):\n",
    "    \"\"\"\n",
    "    Runs the specified community detection algorithm and returns the results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if algorithm_name == \"leiden\":\n",
    "            partition = leidenalg.find_partition(G_ig, leidenalg.ModularityVertexPartition)\n",
    "        elif algorithm_name == \"louvain\":\n",
    "            partition = G_ig.community_multilevel(weights='weight' if 'weight' in G_ig.edge_attributes() else None)\n",
    "        elif algorithm_name == \"label_propagation\":\n",
    "            partition = G_ig.community_label_propagation()\n",
    "        elif algorithm_name == \"fast_greedy\":\n",
    "            partition = G_ig.community_fastgreedy().as_clustering()\n",
    "        elif algorithm_name == \"infomap\":\n",
    "            partition = G_ig.community_infomap()\n",
    "        elif algorithm_name == \"edge_betweenness\":\n",
    "            # Edge betweenness is very slow for large graphs\n",
    "            # Add a safety check here\n",
    "            if G_ig.vcount() > 1000:\n",
    "                print(f\"WARNING: Edge Betweenness algorithm is very slow for graphs with {G_ig.vcount()} nodes.\")\n",
    "                print(\"Press 'e' to skip, or any other key to continue:\")\n",
    "                choice = input().lower()\n",
    "                if choice == 'e':\n",
    "                    return None, None\n",
    "            partition = G_ig.community_edge_betweenness().as_clustering()\n",
    "        else:\n",
    "            print(f\"Unknown algorithm: {algorithm_name}\")\n",
    "            return None, None\n",
    "            \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Convert results to dictionary\n",
    "        community_dict = {}\n",
    "        for i, community in enumerate(partition):\n",
    "            community_dict[i] = list(community)\n",
    "            \n",
    "        return community_dict, elapsed_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while running {algorithm_name}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def save_communities_to_csv(communities, algorithm_name):\n",
    "    \"\"\"\n",
    "    Saves community results to a CSV file\n",
    "    \"\"\"\n",
    "    if not communities:\n",
    "        return False\n",
    "    \n",
    "    # Check/create output directory\n",
    "    output_dir = \"community_results\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create filename\n",
    "    filename = os.path.join(output_dir, f\"{algorithm_name.lower()}_communities.csv\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            \n",
    "            # Write each community as a row\n",
    "            for comm_id, members in communities.items():\n",
    "                row = members\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error while saving CSV file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def print_communities(communities, algorithm_name, elapsed_time):\n",
    "    \"\"\"\n",
    "    Prints community results and saves them to a CSV file\n",
    "    \"\"\"\n",
    "    if not communities:\n",
    "        print(f\"\\n{algorithm_name} results could not be obtained.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{algorithm_name} Results (Time: {elapsed_time:.2f} seconds):\")\n",
    "    print(f\"Total of {len(communities)} communities detected.\")\n",
    "    \n",
    "    # Show only the first 5 communities (in case there are too many)\n",
    "    for i, (comm_id, members) in enumerate(communities.items()):\n",
    "        if i < 5:\n",
    "            print(f\"Community {comm_id}: {len(members)} members\")\n",
    "        else:\n",
    "            print(\"...\")\n",
    "            break\n",
    "    \n",
    "    # Find the largest community\n",
    "    max_community = max(communities.items(), key=lambda x: len(x[1]))\n",
    "    print(f\"Largest community: Community {max_community[0]} ({len(max_community[1])} members)\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    if save_communities_to_csv(communities, algorithm_name):\n",
    "        print(f\"Community results saved to {algorithm_name.lower()}_communities.csv\")\n",
    "    else:\n",
    "        print(f\"WARNING: {algorithm_name} results could not be saved to CSV.\")\n",
    "\n",
    "def main():\n",
    "    # Check/create output directory\n",
    "    output_dir = \"community_results\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"'{output_dir}' directory created.\")\n",
    "    \n",
    "    # Load the graph\n",
    "    print(\"Loading graph...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        G_nx = nx.read_adjlist(\"my.adjlist\")\n",
    "        \n",
    "        # Print graph stats\n",
    "        print(f\"Graph properties:\")\n",
    "        print(f\"Number of nodes: {G_nx.number_of_nodes()}\")\n",
    "        print(f\"Number of edges: {G_nx.number_of_edges()}\")\n",
    "        \n",
    "        # Convert to undirected if needed\n",
    "        if nx.is_directed(G_nx):\n",
    "            G_nx = G_nx.to_undirected()\n",
    "        \n",
    "        # Convert NetworkX graph to iGraph\n",
    "        G_ig = ig.Graph.from_networkx(G_nx)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Graph loaded (Time: {elapsed_time:.2f} seconds)\")\n",
    "        \n",
    "        # Choose algorithms to run\n",
    "        algorithms = [\"leiden\", \"louvain\", \"label_propagation\", \"fast_greedy\", \"infomap\"]\n",
    "        \n",
    "        # Skip edge_betweenness for large graphs\n",
    "        if G_ig.vcount() <= 1000:\n",
    "            algorithms.append(\"edge_betweenness\")\n",
    "        else:\n",
    "            print(\"\\nNOTE: Edge Betweenness algorithm will not be run due to large graph size.\")\n",
    "        \n",
    "        print(\"\\nRunning community detection algorithms...\")\n",
    "        \n",
    "        for algorithm in algorithms:\n",
    "            print(f\"\\nRunning {algorithm.capitalize()} algorithm...\")\n",
    "            communities, elapsed_time = run_algorithm(G_ig, algorithm)\n",
    "            if communities:\n",
    "                print_communities(communities, algorithm.capitalize(), elapsed_time)\n",
    "                \n",
    "        # Alternatively, for parallel execution:\n",
    "        \"\"\"\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            futures = {executor.submit(run_algorithm, G_ig, alg): alg for alg in algorithms}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                algorithm = futures[future]\n",
    "                try:\n",
    "                    communities, elapsed_time = future.result()\n",
    "                    if communities:\n",
    "                        print_communities(communities, algorithm.capitalize(), elapsed_time)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while running {algorithm}: {str(e)}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\nAll tasks completed.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
